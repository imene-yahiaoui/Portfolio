{
  "id": 4,
  "title": "Understanding and Configuring robots.txt",
  "image": "robots-txt.webp",
  "excerpt": "Learn how to create and configure a robots.txt file to control how search engines crawl your site.",
  "content": "<p><img src=\"/assets/images/articles/robot.png\" alt=\"Screenshot of an invalid robots.txt error\" style=\"display:block; margin:20px auto; max-width:100%; border-radius:8px;\"/> The <code>robots.txt</code> file is a key element for controlling how search engines access your site. It allows you to specify which sections of your site should or should not be crawled by bots.</p><h3>1. Why use robots.txt?</h3><p>Here are the main reasons to use a <code>robots.txt</code> file:</p><ul><li><strong>Improve SEO:</strong> Prevent indexing of unnecessary or sensitive pages.</li><li><strong>Reduce server load:</strong> Limit bots' access to non-critical resources.</li><li><strong>Protect sensitive data:</strong> Specify that certain files or directories should not be crawled.</li></ul><h3>2. Example of a robots.txt file</h3><pre><code class=\"language-plaintext\"># Allow all bots to crawl the site\nUser-agent: *\n\n# Block bots from crawling the admin folder\nDisallow: /admin/\n\n# Block specific files\nDisallow: /confidential-data.pdf\n\n# Allow everything except JS and CSS files\nAllow: /static/\nDisallow: /*.js$\nDisallow: /*.css$\n\n# Specify the location of the Sitemap\nSitemap: https://www.example.com/sitemap.xml</code></pre><h3>3. Steps to Configure a robots.txt File</h3><h4>Step 1: Create a robots.txt File</h4><p>Create a file named <code>robots.txt</code> in the root directory of your project. Add the rules necessary to control bots' access.</p><h4>Step 2: Add robots.txt to Your Deployment</h4><p>In a React or Vite project, place the file in the <code>public/</code> directory. When deployed, it will be accessible at the URL <code>https://www.example.com/robots.txt</code>.</p><h4>Step 3: Test Your robots.txt File</h4><p>Use the tool <a href=\"https://technicalseo.com/tools/robots-txt/\" target=\"_blank\">robots.txt Validator and Testing Tool</a> to ensure your configuration is correct. <img src=\"/assets/images/articles/chekRobot.PNG\" alt=\"Screenshot of robots.txt validation\" style=\"display:block; margin:20px auto; max-width:100%; border-radius:8px;\"/></p><h3>4. Best Practices for robots.txt</h3><ul><li><strong>Don't block essential files:</strong> Ensure that necessary CSS and JS files are not blocked.</li><li><strong>Add a Sitemap:</strong> Specify the location of your XML Sitemap. For more information, refer to <a href=\"/article/5/\" target=\"_blank\">Create and Add an XML Sitemap</a>.</li><li><strong>Avoid using robots.txt to secure sensitive data:</strong> Blocked files can still be accessed via direct URLs.</li></ul><h3>5. Example for a React Project</h3><pre><code class=\"language-plaintext\">User-agent: *\nDisallow: /private/\nDisallow: /admin/\nAllow: /static/\nSitemap: https://www.myreactapp.com/sitemap.xml</code></pre><h3>Conclusion</h3><p>A well-configured <code>robots.txt</code> file ensures that search engines crawl only the relevant parts of your site. This improves your SEO while protecting your resources.</p>"
}
